{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f924b101-1ad2-4f06-9210-a3afae871cd9",
   "metadata": {},
   "source": [
    "# Phase 2: Feature Exploration & Analysis\n",
    "\n",
    "## Objectives\n",
    "1. Systematically explore features from the pre-trained SAE\n",
    "2. Find max-activating examples for top features\n",
    "3. Visualize feature activation patterns across diverse texts\n",
    "4. Build interactive exploration tools\n",
    "5. Analyze feature co-activation patterns\n",
    "6. Integrate with Neuronpedia for feature interpretation\n",
    "\n",
    "## What We'll Learn\n",
    "- How to systematically analyze SAE features\n",
    "- What makes specific features activate\n",
    "- How to interpret feature meanings from activation patterns\n",
    "- Which features co-activate (fire together)\n",
    "- How to build reusable exploration tools\n",
    "\n",
    "## Prerequisites\n",
    "- Completed Phase 1 (model loaded, SAE loaded, initial activations cached)\n",
    "- Cached data: `../data/phase1_activations.pt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4badf218-76b4-42c6-aa81-cec2181a66df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All libraries imported successfully!\n",
      "PyTorch version: 2.9.0+cpu\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: Import Libraries\n",
    "# ============================================================================\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TransformerLens and SAELens\n",
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceebe65a-4968-4e5a-9647-2c286c17f12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Loading GPT-2 and SAE...\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "âœ… Model loaded: gpt2\n",
      "\n",
      "ğŸ“š Available SAEs:\n",
      "  1. 6-res-jb: Layer 6 Residual Stream (Joseph Bloom)\n",
      "  2. 8-res-jb: Layer 8 Residual Stream (Joseph Bloom)\n",
      "  3. 10-res-jb: Layer 10 Residual Stream (Joseph Bloom)\n",
      "  4. 6-mlp-out: Layer 6 MLP Output\n",
      "\n",
      "ğŸ¯ Selected SAE: 6-res-jb\n",
      "   Description: Layer 6 Residual Stream (Joseph Bloom)\n",
      "\n",
      "âœ… SAE loaded successfully!\n",
      "   Architecture: 768 â†’ 24576 features\n",
      "   Hook point: blocks.6.hook_resid_pre\n",
      "\n",
      "======================================================================\n",
      "âœ… Setup complete! Ready to analyze features.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: Load LLM & SAE (with Multiple SAE Options)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"ğŸ”§ Loading GPT-2 and SAE...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load model (this stays the same)\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"âœ… Model loaded: {model.cfg.model_name}\")\n",
    "\n",
    "# SAE Selection\n",
    "# Available SAEs for GPT2-Small\n",
    "available_saes = {\n",
    "    \"6-res-jb\": {\n",
    "        \"description\": \"Layer 6 Residual Stream (Joseph Bloom)\",\n",
    "        \"hook_point\": \"blocks.6.hook_resid_pre\",\n",
    "        \"path\": \"~/.cache/sae_lens/blocks.6.hook_resid_pre\",\n",
    "        \"d_in\": 768,\n",
    "        \"d_sae\": 24576\n",
    "    },\n",
    "    \"8-res-jb\": {\n",
    "        \"description\": \"Layer 8 Residual Stream (Joseph Bloom)\",\n",
    "        \"hook_point\": \"blocks.8.hook_resid_pre\",\n",
    "        \"path\": \"~/.cache/sae_lens/blocks.8.hook_resid_pre\",\n",
    "        \"d_in\": 768,\n",
    "        \"d_sae\": 24576\n",
    "    },\n",
    "    \"10-res-jb\": {\n",
    "        \"description\": \"Layer 10 Residual Stream (Joseph Bloom)\",\n",
    "        \"hook_point\": \"blocks.10.hook_resid_pre\",\n",
    "        \"path\": \"~/.cache/sae_lens/blocks.10.hook_resid_pre\",\n",
    "        \"d_in\": 768,\n",
    "        \"d_sae\": 24576\n",
    "    },\n",
    "    \"6-mlp-out\": {\n",
    "        \"description\": \"Layer 6 MLP Output\",\n",
    "        \"hook_point\": \"blocks.6.hook_mlp_out\",\n",
    "        \"path\": \"~/.cache/sae_lens/blocks.6.hook_mlp_out\",\n",
    "        \"d_in\": 768,\n",
    "        \"d_sae\": 24576\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display available SAEs\n",
    "print(\"\\nğŸ“š Available SAEs:\")\n",
    "for idx, (key, info) in enumerate(available_saes.items(), 1):\n",
    "    print(f\"  {idx}. {key}: {info['description']}\")\n",
    "\n",
    "# SELECT WHICH SAE TO USE HERE:\n",
    "selected_sae = \"6-res-jb\"  # Change this to select different SAE\n",
    "\n",
    "print(f\"\\nğŸ¯ Selected SAE: {selected_sae}\")\n",
    "print(f\"   Description: {available_saes[selected_sae]['description']}\")\n",
    "\n",
    "# Load the selected SAE\n",
    "sae_info = available_saes[selected_sae]\n",
    "hook_name = sae_info[\"hook_point\"]\n",
    "\n",
    "cache_path = os.path.expanduser(sae_info[\"path\"])\n",
    "\n",
    "try:\n",
    "    sae = SAE.load_from_disk(path=cache_path, device=\"cpu\")\n",
    "    print(f\"\\nâœ… SAE loaded successfully!\")\n",
    "    print(f\"   Architecture: {sae_info['d_in']} â†’ {sae_info['d_sae']} features\")\n",
    "    print(f\"   Hook point: {hook_name}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nâš ï¸  SAE not found at: {cache_path}\")\n",
    "    print(f\"   You may need to download it first using SAELens.\")\n",
    "    print(f\"   Example: SAE.from_pretrained(...)\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… Setup complete! Ready to analyze features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ffc3ad2-2519-4f4a-9c4f-a0f87e706d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Extracting activations and features from sample texts...\n",
      "======================================================================\n",
      "Analyzing 4 initial texts...\n",
      "\n",
      "'The Eiffel Tower is in Paris'\n",
      "Top 5 features:\n",
      "  Feature #9191: 47.03\n",
      "  Feature #8654: 13.49\n",
      "  Feature #10032: 5.68\n",
      "  Feature #5656: 4.01\n",
      "  Feature #8167: 3.67\n",
      "\n",
      "'Python is a programming language'\n",
      "Top 5 features:\n",
      "  Feature #1676: 28.07\n",
      "  Feature #17833: 21.14\n",
      "  Feature #5816: 9.91\n",
      "  Feature #12215: 8.82\n",
      "  Feature #11825: 4.76\n",
      "\n",
      "'The cat sat on the mat'\n",
      "Top 5 features:\n",
      "  Feature #19196: 49.34\n",
      "  Feature #14904: 7.95\n",
      "  Feature #20572: 7.63\n",
      "  Feature #7388: 5.74\n",
      "  Feature #15413: 5.52\n",
      "\n",
      "'Machine learning models learn patterns'\n",
      "Top 5 features:\n",
      "  Feature #23421: 33.08\n",
      "  Feature #7777: 6.19\n",
      "  Feature #15821: 5.54\n",
      "  Feature #15989: 5.00\n",
      "  Feature #24559: 3.80\n",
      "\n",
      "ğŸ“Š Total feature activations: torch.Size([4, 24576])\n",
      "   â†’ 4 texts analyzed\n",
      "   â†’ 24576 features per text (most are zero due to sparsity)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: Extract Features from Texts\n",
    "# ============================================================================\n",
    "\n",
    "print(\"ğŸ” Extracting activations and features from sample texts...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Sample texts to analyze\n",
    "texts = [\n",
    "    \"The Eiffel Tower is in Paris\",\n",
    "    \"Python is a programming language\",\n",
    "    \"The cat sat on the mat\",\n",
    "    \"Machine learning models learn patterns\",\n",
    "]\n",
    "\n",
    "print(f\"Analyzing {len(texts)} initial texts...\\n\")\n",
    "\n",
    "# Extract activations and features\n",
    "all_features = []\n",
    "\n",
    "for text in texts:\n",
    "    # Extract activation from model\n",
    "    logits, cache = model.run_with_cache(text)\n",
    "    acts = cache[hook_name][0, -1, :]  # Last token activation\n",
    "    \n",
    "    # Get SAE features\n",
    "    with torch.no_grad():\n",
    "        features = sae.encode(acts.unsqueeze(0))\n",
    "        all_features.append(features[0])\n",
    "        \n",
    "        # Show top 5 for each text\n",
    "        top_vals, top_indices = features[0].topk(5)\n",
    "        print(f\"'{text}'\")\n",
    "        print(f\"Top 5 features:\")\n",
    "        for idx, val in zip(top_indices, top_vals):\n",
    "            print(f\"  Feature #{idx.item()}: {val.item():.2f}\")\n",
    "        print()\n",
    "\n",
    "# Stack for analysis\n",
    "all_features = torch.stack(all_features)\n",
    "print(f\"ğŸ“Š Total feature activations: {all_features.shape}\")\n",
    "print(f\"   â†’ {all_features.shape[0]} texts analyzed\")\n",
    "print(f\"   â†’ {all_features.shape[1]} features per text (most are zero due to sparsity)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaf496c5-19d7-485e-9a1c-1a0f5796faa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ Finding Most Frequently Active Features Across All Texts\n",
      "======================================================================\n",
      "\n",
      "Top 20 Most Frequently Active Features:\n",
      "  1. Feature #3639: active in 4/4 texts\n",
      "  2. Feature #5766: active in 4/4 texts\n",
      "  3. Feature #19740: active in 3/4 texts\n",
      "  4. Feature #8774: active in 3/4 texts\n",
      "  5. Feature #1622: active in 3/4 texts\n",
      "  6. Feature #19960: active in 2/4 texts\n",
      "  7. Feature #18146: active in 2/4 texts\n",
      "  8. Feature #17787: active in 2/4 texts\n",
      "  9. Feature #12873: active in 2/4 texts\n",
      "  10. Feature #22371: active in 2/4 texts\n",
      "  11. Feature #14556: active in 2/4 texts\n",
      "  12. Feature #12215: active in 2/4 texts\n",
      "  13. Feature #20957: active in 2/4 texts\n",
      "  14. Feature #21631: active in 2/4 texts\n",
      "  15. Feature #21869: active in 2/4 texts\n",
      "  16. Feature #6861: active in 2/4 texts\n",
      "  17. Feature #13178: active in 2/4 texts\n",
      "  18. Feature #23227: active in 2/4 texts\n",
      "  19. Feature #11114: active in 2/4 texts\n",
      "  20. Feature #4729: active in 2/4 texts\n",
      "\n",
      "ğŸ’¡ Interpretation:\n",
      "   â†’ 2 of the top 20 features activate in ALL texts\n",
      "   â†’ These are likely 'universal' features representing:\n",
      "      â€¢ Basic syntactic structures\n",
      "      â€¢ Common grammatical patterns\n",
      "      â€¢ General semantic processing\n",
      "   â†’ Later we'll find 'selective' features that distinguish content types\n",
      "\n",
      "ğŸ’¾ Saved top 10 for heatmap visualization: [3639, 5766, 19740, 8774, 1622, 19960, 18146, 17787, 12873, 22371]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: Find Most Frequently Active Features\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nğŸ¯ Finding Most Frequently Active Features Across All Texts\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Count how many times each feature is active\n",
    "feature_counts = (all_features > 0).sum(dim=0)\n",
    "\n",
    "# Get top most frequent features\n",
    "most_freq_features = 20\n",
    "top_frequent_features = feature_counts.topk(most_freq_features)\n",
    "\n",
    "print(f\"\\nTop {most_freq_features} Most Frequently Active Features:\")\n",
    "for rank, (count, idx) in enumerate(zip(top_frequent_features.values, top_frequent_features.indices), 1):\n",
    "    print(f\"  {rank}. Feature #{idx.item()}: active in {count.item()}/{len(texts)} texts\")\n",
    "\n",
    "# Add interpretation\n",
    "print(\"\\nğŸ’¡ Interpretation:\")\n",
    "all_active_count = sum(1 for count in top_frequent_features.values if count.item() == len(texts))\n",
    "if all_active_count > 0:\n",
    "    print(f\"   â†’ {all_active_count} of the top {most_freq_features} features activate in ALL texts\")\n",
    "    print(f\"   â†’ These are likely 'universal' features representing:\")\n",
    "    print(f\"      â€¢ Basic syntactic structures\")\n",
    "    print(f\"      â€¢ Common grammatical patterns\")\n",
    "    print(f\"      â€¢ General semantic processing\")\n",
    "    print(f\"   â†’ Later we'll find 'selective' features that distinguish content types\")\n",
    "else:\n",
    "    print(f\"   â†’ Features show varying activation patterns across texts\")\n",
    "    print(f\"   â†’ This suggests more specialized feature behavior\")\n",
    "\n",
    "# Save top 10 for visualization\n",
    "frequent_features = top_frequent_features.indices[:10].tolist()\n",
    "print(f\"\\nğŸ’¾ Saved top 10 for heatmap visualization: {frequent_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "689657cf-539e-4caa-bd42-35f933b8bb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ§ª Loading Larger Diverse Test Dataset\n",
      "======================================================================\n",
      "Loaded 70 diverse texts across 7 categories:\n",
      "   â†’ 10 Python code examples\n",
      "   â†’ 10 URLs/web content\n",
      "   â†’ 10 Mathematical notation\n",
      "   â†’ 10 Non-English languages\n",
      "   â†’ 10 Social media/casual with emojis\n",
      "   â†’ 10 Formal/academic writing\n",
      "   â†’ 10 Regular conversational English\n",
      "\n",
      "This should reveal more specialized features!\n",
      "\n",
      "âœ… Extracted features for 70 texts\n",
      "   Feature tensor shape: torch.Size([70, 24576])\n",
      "   â†’ 70 texts analyzed\n",
      "   â†’ 24576 features per text\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: Load Larger Diverse Dataset\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nğŸ§ª Loading Larger Diverse Test Dataset\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create a much larger and more diverse dataset\n",
    "diverse_texts = [\n",
    "    # Python code (10 examples)\n",
    "    \"def factorial(n):\\n    return 1 if n == 0 else n * factorial(n-1)\",\n",
    "    \"import torch\\nimport numpy as np\\nfrom transformers import AutoModel\",\n",
    "    \"class NeuralNetwork(nn.Module):\\n    def __init__(self):\",\n",
    "    \"for i in range(len(data)):\\n    result.append(data[i] ** 2)\",\n",
    "    \"try:\\n    x = int(input())\\nexcept ValueError:\\n    print('Error')\",\n",
    "    \"lambda x: x ** 2 + 3 * x - 5\",\n",
    "    \"if __name__ == '__main__':\\n    main()\",\n",
    "    \"return [x for x in lst if x > 0]\",\n",
    "    \"print(f'Result: {sum(values) / len(values):.2f}')\",\n",
    "    \"pip install transformers torch numpy pandas\",\n",
    "    \n",
    "    # URLs and web content (10 examples)\n",
    "    \"https://www.github.com/anthropics/claude\",\n",
    "    \"Visit our website at http://example.com/products\",\n",
    "    \"<html><body><h1>Welcome</h1></body></html>\",\n",
    "    \"<div class='container'><p>Content here</p></div>\",\n",
    "    \"GET /api/v1/users HTTP/1.1\",\n",
    "    \"mailto:support@example.com\",\n",
    "    \"www.stackoverflow.com/questions/12345\",\n",
    "    \"ftp://files.example.org/downloads/\",\n",
    "    \"Click here: https://bit.ly/abc123\",\n",
    "    \"Check out reddit.com/r/machinelearning\",\n",
    "    \n",
    "    # Mathematical notation (10 examples)\n",
    "    \"f(x) = x^2 + 2x + 1\",\n",
    "    \"âˆ«(x^2 + 3x)dx = x^3/3 + 3x^2/2 + C\",\n",
    "    \"lim(xâ†’0) sin(x)/x = 1\",\n",
    "    \"âˆ‘(i=1 to n) i = n(n+1)/2\",\n",
    "    \"âˆš(a^2 + b^2) = c\",\n",
    "    \"P(A|B) = P(B|A)P(A) / P(B)\",\n",
    "    \"E = mc^2\",\n",
    "    \"âˆ‡f(x,y) = (âˆ‚f/âˆ‚x, âˆ‚f/âˆ‚y)\",\n",
    "    \"det([[a,b],[c,d]]) = ad - bc\",\n",
    "    \"sin^2(Î¸) + cos^2(Î¸) = 1\",\n",
    "    \n",
    "    # Non-English languages (10 examples)\n",
    "    \"Bonjour, comment allez-vous aujourd'hui?\",\n",
    "    \"ä½ å¥½ï¼Œä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ\",\n",
    "    \"Hola, Â¿cÃ³mo estÃ¡s?\",\n",
    "    \"Guten Tag, wie geht es Ihnen?\",\n",
    "    \"Ğ—Ğ´Ñ€Ğ°Ğ²ÑÑ‚Ğ²ÑƒĞ¹Ñ‚Ğµ, ĞºĞ°Ğº Ğ´ĞµĞ»Ğ°?\",\n",
    "    \"ã“ã‚“ã«ã¡ã¯ã€å…ƒæ°—ã§ã™ã‹ï¼Ÿ\",\n",
    "    \"Ù…Ø±Ø­Ø¨Ø§ØŒ ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ\",\n",
    "    \"ì•ˆë…•í•˜ì„¸ìš”, ì˜ ì§€ë‚´ì…¨ì–´ìš”?\",\n",
    "    \"Ciao, come stai?\",\n",
    "    \"OlÃ¡, como vocÃª estÃ¡?\",\n",
    "    \n",
    "    # Social media/casual with emojis (10 examples)\n",
    "    \"omg that's so funny ğŸ˜‚ğŸ˜‚ğŸ˜‚\",\n",
    "    \"can't wait for the weekend!! ğŸ‰ğŸŠ\",\n",
    "    \"just got coffee â˜• feeling good âœ¨\",\n",
    "    \"bruh why is this happening ğŸ’€\",\n",
    "    \"yaaaas queen!!! ğŸ‘‘ğŸ’…âœ¨\",\n",
    "    \"ngl this is pretty cool ğŸ”¥\",\n",
    "    \"lmaooo i'm dying ğŸ˜­ğŸ˜­\",\n",
    "    \"tbh idk what to do ğŸ¤·â€â™€ï¸\",\n",
    "    \"mood af rn ğŸ’¯\",\n",
    "    \"this slaps fr fr ğŸµğŸ”¥\",\n",
    "    \n",
    "    # Formal/academic (10 examples)\n",
    "    \"The phenomenon was observed under controlled laboratory conditions.\",\n",
    "    \"In accordance with the aforementioned regulations, we hereby submit this proposal.\",\n",
    "    \"The hypothesis was tested using a double-blind randomized controlled trial.\",\n",
    "    \"Pursuant to Article 12, Section 3 of the aforementioned statute.\",\n",
    "    \"The results indicate a statistically significant correlation (p < 0.05).\",\n",
    "    \"This paper examines the theoretical frameworks underlying modern economics.\",\n",
    "    \"The defendant pleaded not guilty to all charges in the indictment.\",\n",
    "    \"We acknowledge the contributions of all co-authors and funding agencies.\",\n",
    "    \"The experimental methodology followed established protocols.\",\n",
    "    \"In conclusion, further research is warranted to investigate this phenomenon.\",\n",
    "    \n",
    "    # Regular conversational English (10 examples)\n",
    "    \"Hey, what's up? Want to grab lunch later?\",\n",
    "    \"I think the meeting went pretty well today.\",\n",
    "    \"The weather is nice, maybe we should go for a walk.\",\n",
    "    \"Did you see that movie everyone's talking about?\",\n",
    "    \"I'm planning a trip to Japan next summer.\",\n",
    "    \"That restaurant has the best pizza in town.\",\n",
    "    \"My cat keeps knocking things off the table.\",\n",
    "    \"The traffic was terrible this morning.\",\n",
    "    \"I need to finish this project by Friday.\",\n",
    "    \"Let's catch up over coffee sometime.\",\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(diverse_texts)} diverse texts across 7 categories:\")\n",
    "print(f\"   â†’ 10 Python code examples\")\n",
    "print(f\"   â†’ 10 URLs/web content\")\n",
    "print(f\"   â†’ 10 Mathematical notation\")\n",
    "print(f\"   â†’ 10 Non-English languages\")\n",
    "print(f\"   â†’ 10 Social media/casual with emojis\")\n",
    "print(f\"   â†’ 10 Formal/academic writing\")\n",
    "print(f\"   â†’ 10 Regular conversational English\")\n",
    "print(f\"\\nThis should reveal more specialized features!\\n\")\n",
    "\n",
    "# Extract features for all diverse texts using BATCHED processing\n",
    "# This is much more efficient than processing texts one at a time\n",
    "print(\"Using batched text processing for efficiency...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Tokenize all texts at once\n",
    "    tokens = model.to_tokens(diverse_texts, prepend_bos=True)\n",
    "    \n",
    "    # Run model once for all texts (batched forward pass)\n",
    "    _, cache = model.run_with_cache(tokens, names_filter=[hook_name])\n",
    "    \n",
    "    # Get activations for all texts at once (last ACTUAL token, not padding)\n",
    "    # Shape: [num_texts, d_model]\n",
    "    # Note: When batching, shorter sequences are padded. We need to get the last real token.\n",
    "    pad_token_id = model.tokenizer.pad_token_id if model.tokenizer.pad_token_id is not None else 0\n",
    "    seq_lens = (tokens != pad_token_id).sum(dim=1)  # Length of each sequence (excluding padding)\n",
    "    activations = torch.stack([cache[hook_name][i, seq_lens[i]-1, :] for i in range(len(diverse_texts))])\n",
    "    \n",
    "    # Encode all activations through SAE at once\n",
    "    # Shape: [num_texts, d_sae]\n",
    "    diverse_features = sae.encode(activations)\n",
    "\n",
    "print(f\"Batched processing complete!\")\n",
    "\n",
    "print(f\"âœ… Extracted features for {len(diverse_texts)} texts\")\n",
    "print(f\"   Feature tensor shape: {diverse_features.shape}\")\n",
    "print(f\"   â†’ {diverse_features.shape[0]} texts analyzed\")\n",
    "print(f\"   â†’ {diverse_features.shape[1]} features per text\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0f88668-3b1c-4ae3-95fc-5ec3b861bfc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¬ Finding and Analyzing Most Interesting Features\n",
      "======================================================================\n",
      "\n",
      "1ï¸âƒ£  STRONGEST Feature (highest activation value):\n",
      "   Feature #23575\n",
      "   Max activation: 48.83\n",
      "\n",
      "2ï¸âƒ£  MOST FREQUENT Feature (active in most texts):\n",
      "   Feature #18043\n",
      "   Active in: 33/70 texts\n",
      "\n",
      "3ï¸âƒ£  MOST SELECTIVE Feature (specialist - high activation, few texts):\n",
      "   Feature #295\n",
      "   Max activation: 6.28\n",
      "   Strong activations (>5.0) in: 1/70 texts (1.4%)\n",
      "   Any activation (>0) in: 2/70 texts\n",
      "\n",
      "ğŸ’¡ Analyzing 3 distinct features in detail:\n",
      "   â†’ Feature #23575 (STRONGEST)\n",
      "   â†’ Feature #18043 (MOST FREQUENT)\n",
      "   â†’ Feature #295 (MOST SELECTIVE)\n",
      "\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š Feature #23575 (STRONGEST)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Top 10 activating texts:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Activation</th>\n",
       "      <th>Active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>&lt;div class='container'&gt;&lt;p&gt;Content here&lt;/p&gt;&lt;/div&gt;</td>\n",
       "      <td>48.826561</td>\n",
       "      <td>âœ“</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>&lt;html&gt;&lt;body&gt;&lt;h1&gt;Welcome&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;</td>\n",
       "      <td>44.523075</td>\n",
       "      <td>âœ“</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>this slaps fr fr ğŸµğŸ”¥</td>\n",
       "      <td>0.600993</td>\n",
       "      <td>âœ“</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>In accordance with the aforementioned regulati...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>âœ—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>The phenomenon was observed under controlled l...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>âœ—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>mood af rn ğŸ’¯</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>âœ—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>tbh idk what to do ğŸ¤·â€â™€ï¸</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>âœ—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>lmaooo i'm dying ğŸ˜­ğŸ˜­</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>âœ—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>ngl this is pretty cool ğŸ”¥</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>âœ—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>The hypothesis was tested using a double-blind...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>âœ—</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Text  Activation Active\n",
       "13   <div class='container'><p>Content here</p></div>   48.826561      âœ“\n",
       "12         <html><body><h1>Welcome</h1></body></html>   44.523075      âœ“\n",
       "49                                this slaps fr fr ğŸµğŸ”¥    0.600993      âœ“\n",
       "51  In accordance with the aforementioned regulati...    0.000000      âœ—\n",
       "50  The phenomenon was observed under controlled l...    0.000000      âœ—\n",
       "48                                       mood af rn ğŸ’¯    0.000000      âœ—\n",
       "47                            tbh idk what to do ğŸ¤·â€â™€ï¸    0.000000      âœ—\n",
       "46                                lmaooo i'm dying ğŸ˜­ğŸ˜­    0.000000      âœ—\n",
       "45                          ngl this is pretty cool ğŸ”¥    0.000000      âœ—\n",
       "52  The hypothesis was tested using a double-blind...    0.000000      âœ—"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ˆ Statistics:\n",
      "   Active in: 3/70 texts (4.3%)\n",
      "   Mean activation (when active): 31.317\n",
      "   Max activation: 48.827\n",
      "\n",
      "ğŸ”— Explore on Neuronpedia:\n",
      "   https://neuronpedia.org/gpt2-small/6-res-jb/23575\n",
      "\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š Feature #18043 (MOST FREQUENT)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Top 10 activating texts:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Activation</th>\n",
       "      <th>Active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>âˆ«(x^2 + 3x)dx = x^3/3 + 3x^2/2 + C</td>\n",
       "      <td>7.019273</td>\n",
       "      <td>âœ“</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>âˆš(a^2 + b^2) = c</td>\n",
       "      <td>6.319774</td>\n",
       "      <td>âœ“</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>sin^2(Î¸) + cos^2(Î¸) = 1</td>\n",
       "      <td>5.162304</td>\n",
       "      <td>âœ“</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>ä½ å¥½ï¼Œä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ</td>\n",
       "      <td>4.968639</td>\n",
       "      <td>âœ“</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>for i in range(len(data)):\\n    result.append(...</td>\n",
       "      <td>4.471064</td>\n",
       "      <td>âœ“</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>lim(xâ†’0) sin(x)/x = 1</td>\n",
       "      <td>4.428233</td>\n",
       "      <td>âœ“</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>print(f'Result: {sum(values) / len(values):.2f}')</td>\n",
       "      <td>3.705222</td>\n",
       "      <td>âœ“</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>yaaaas queen!!! ğŸ‘‘ğŸ’…âœ¨</td>\n",
       "      <td>3.700651</td>\n",
       "      <td>âœ“</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>ì•ˆë…•í•˜ì„¸ìš”, ì˜ ì§€ë‚´ì…¨ì–´ìš”?</td>\n",
       "      <td>3.357204</td>\n",
       "      <td>âœ“</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>âˆ‡f(x,y) = (âˆ‚f/âˆ‚x, âˆ‚f/âˆ‚y)</td>\n",
       "      <td>2.484785</td>\n",
       "      <td>âœ“</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Text  Activation Active\n",
       "21                 âˆ«(x^2 + 3x)dx = x^3/3 + 3x^2/2 + C    7.019273      âœ“\n",
       "24                                   âˆš(a^2 + b^2) = c    6.319774      âœ“\n",
       "29                            sin^2(Î¸) + cos^2(Î¸) = 1    5.162304      âœ“\n",
       "31                                        ä½ å¥½ï¼Œä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ    4.968639      âœ“\n",
       "3   for i in range(len(data)):\\n    result.append(...    4.471064      âœ“\n",
       "22                              lim(xâ†’0) sin(x)/x = 1    4.428233      âœ“\n",
       "8   print(f'Result: {sum(values) / len(values):.2f}')    3.705222      âœ“\n",
       "44                                yaaaas queen!!! ğŸ‘‘ğŸ’…âœ¨    3.700651      âœ“\n",
       "37                                    ì•ˆë…•í•˜ì„¸ìš”, ì˜ ì§€ë‚´ì…¨ì–´ìš”?    3.357204      âœ“\n",
       "27                           âˆ‡f(x,y) = (âˆ‚f/âˆ‚x, âˆ‚f/âˆ‚y)    2.484785      âœ“"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ˆ Statistics:\n",
      "   Active in: 33/70 texts (47.1%)\n",
      "   Mean activation (when active): 1.949\n",
      "   Max activation: 7.019\n",
      "\n",
      "ğŸ”— Explore on Neuronpedia:\n",
      "   https://neuronpedia.org/gpt2-small/6-res-jb/18043\n",
      "\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š Feature #295 (MOST SELECTIVE)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Top 10 activating texts:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Activation</th>\n",
       "      <th>Active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>GET /api/v1/users HTTP/1.1</td>\n",
       "      <td>6.282235</td>\n",
       "      <td>âœ“</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>can't wait for the weekend!! ğŸ‰ğŸŠ</td>\n",
       "      <td>0.041764</td>\n",
       "      <td>âœ“</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>In accordance with the aforementioned regulati...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>âœ—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>The phenomenon was observed under controlled l...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>âœ—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>this slaps fr fr ğŸµğŸ”¥</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>âœ—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>mood af rn ğŸ’¯</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>âœ—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>tbh idk what to do ğŸ¤·â€â™€ï¸</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>âœ—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>lmaooo i'm dying ğŸ˜­ğŸ˜­</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>âœ—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>ngl this is pretty cool ğŸ”¥</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>âœ—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>ã“ã‚“ã«ã¡ã¯ã€å…ƒæ°—ã§ã™ã‹ï¼Ÿ</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>âœ—</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Text  Activation Active\n",
       "14                         GET /api/v1/users HTTP/1.1    6.282235      âœ“\n",
       "41                    can't wait for the weekend!! ğŸ‰ğŸŠ    0.041764      âœ“\n",
       "51  In accordance with the aforementioned regulati...    0.000000      âœ—\n",
       "50  The phenomenon was observed under controlled l...    0.000000      âœ—\n",
       "49                                this slaps fr fr ğŸµğŸ”¥    0.000000      âœ—\n",
       "48                                       mood af rn ğŸ’¯    0.000000      âœ—\n",
       "47                            tbh idk what to do ğŸ¤·â€â™€ï¸    0.000000      âœ—\n",
       "46                                lmaooo i'm dying ğŸ˜­ğŸ˜­    0.000000      âœ—\n",
       "45                          ngl this is pretty cool ğŸ”¥    0.000000      âœ—\n",
       "35                                       ã“ã‚“ã«ã¡ã¯ã€å…ƒæ°—ã§ã™ã‹ï¼Ÿ    0.000000      âœ—"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ˆ Statistics:\n",
      "   Active in: 2/70 texts (2.9%)\n",
      "   Mean activation (when active): 3.162\n",
      "   Max activation: 6.282\n",
      "\n",
      "ğŸ”— Explore on Neuronpedia:\n",
      "   https://neuronpedia.org/gpt2-small/6-res-jb/295\n",
      "\n",
      "======================================================================\n",
      "\n",
      "4ï¸âƒ£  CATEGORY-SPECIFIC FEATURE ANALYSIS (by MAX activation)\n",
      "======================================================================\n",
      "For each category, finding features with highest activation\n",
      "and checking if they specialize in that category (activate strongly\n",
      "inside the category but rarely outside it):\n",
      "\n",
      "\n",
      "Python:\n",
      "  1. Feature #7155: max=38.02, strong(>5.0) inside=1/10, outside=0/60\n",
      "  2. Feature #14848: max=30.14, strong(>5.0) inside=1/10, outside=0/60\n",
      "  3. Feature #3252: max=28.41, strong(>5.0) inside=1/10, outside=0/60\n",
      "  4. Feature #17352: max=25.87, strong(>5.0) inside=1/10, outside=0/60\n",
      "  5. Feature #3197: max=25.71, strong(>5.0) inside=6/10, outside=2/60\n",
      "  âœ“ True specialist found: Feature #3197 (score: 4)\n",
      "  ğŸ”— Top feature for Python: https://neuronpedia.org/gpt2-small/6-res-jb/3197\n",
      "\n",
      "URLs/Web:\n",
      "  1. Feature #23575: max=48.83, strong(>5.0) inside=2/10, outside=0/60\n",
      "  2. Feature #17510: max=30.14, strong(>5.0) inside=1/10, outside=0/60\n",
      "  3. Feature #1747: max=29.74, strong(>5.0) inside=1/10, outside=0/60\n",
      "  4. Feature #22237: max=22.73, strong(>5.0) inside=1/10, outside=0/60\n",
      "  5. Feature #4073: max=21.86, strong(>5.0) inside=1/10, outside=0/60\n",
      "  âœ“ True specialist found: Feature #23575 (score: 2)\n",
      "  ğŸ”— Top feature for URLs/Web: https://neuronpedia.org/gpt2-small/6-res-jb/23575\n",
      "\n",
      "Math:\n",
      "  1. Feature #6210: max=38.58, strong(>5.0) inside=1/10, outside=0/60\n",
      "  2. Feature #16704: max=31.95, strong(>5.0) inside=1/10, outside=0/60\n",
      "  3. Feature #13442: max=24.73, strong(>5.0) inside=3/10, outside=0/60\n",
      "  4. Feature #14289: max=23.50, strong(>5.0) inside=2/10, outside=0/60\n",
      "  5. Feature #623: max=13.79, strong(>5.0) inside=1/10, outside=2/60\n",
      "  âœ“ True specialist found: Feature #13442 (score: 3)\n",
      "  ğŸ”— Top feature for Math: https://neuronpedia.org/gpt2-small/6-res-jb/13442\n",
      "\n",
      "Non-English:\n",
      "  1. Feature #4186: max=18.76, strong(>5.0) inside=3/10, outside=5/60\n",
      "  2. Feature #11722: max=15.69, strong(>5.0) inside=2/10, outside=10/60\n",
      "  3. Feature #16247: max=13.88, strong(>5.0) inside=6/10, outside=0/60\n",
      "  4. Feature #8545: max=13.61, strong(>5.0) inside=7/10, outside=1/60\n",
      "  5. Feature #7484: max=13.59, strong(>5.0) inside=7/10, outside=2/60\n",
      "  âœ“ True specialist found: Feature #16247 (score: 6)\n",
      "  ğŸ”— Top feature for Non-English: https://neuronpedia.org/gpt2-small/6-res-jb/16247\n",
      "\n",
      "Social/Emoji:\n",
      "  1. Feature #17019: max=27.46, strong(>5.0) inside=1/10, outside=0/60\n",
      "  2. Feature #4301: max=26.46, strong(>5.0) inside=2/10, outside=0/60\n",
      "  3. Feature #11722: max=19.02, strong(>5.0) inside=10/10, outside=2/60\n",
      "  4. Feature #2413: max=12.14, strong(>5.0) inside=1/10, outside=0/60\n",
      "  5. Feature #13865: max=11.56, strong(>5.0) inside=1/10, outside=0/60\n",
      "  âœ“ True specialist found: Feature #11722 (score: 8)\n",
      "  ğŸ”— Top feature for Social/Emoji: https://neuronpedia.org/gpt2-small/6-res-jb/11722\n",
      "\n",
      "Formal:\n",
      "  1. Feature #16620: max=16.97, strong(>5.0) inside=1/10, outside=0/60\n",
      "  2. Feature #21309: max=10.60, strong(>5.0) inside=8/10, outside=8/60\n",
      "  3. Feature #17363: max=10.11, strong(>5.0) inside=5/10, outside=0/60\n",
      "  4. Feature #10375: max=7.72, strong(>5.0) inside=1/10, outside=0/60\n",
      "  5. Feature #12013: max=7.43, strong(>5.0) inside=1/10, outside=0/60\n",
      "  âœ“ True specialist found: Feature #17363 (score: 5)\n",
      "  ğŸ”— Top feature for Formal: https://neuronpedia.org/gpt2-small/6-res-jb/17363\n",
      "\n",
      "Conversational:\n",
      "  1. Feature #21436: max=19.35, strong(>5.0) inside=2/10, outside=0/60\n",
      "  2. Feature #1753: max=9.94, strong(>5.0) inside=1/10, outside=1/60\n",
      "  3. Feature #19999: max=9.76, strong(>5.0) inside=3/10, outside=0/60\n",
      "  4. Feature #11542: max=8.89, strong(>5.0) inside=5/10, outside=0/60\n",
      "  5. Feature #21309: max=8.20, strong(>5.0) inside=8/10, outside=8/60\n",
      "  âœ“ True specialist found: Feature #11542 (score: 5)\n",
      "  ğŸ”— Top feature for Conversational: https://neuronpedia.org/gpt2-small/6-res-jb/11542\n",
      "\n",
      "======================================================================\n",
      "\n",
      "ğŸ¯ Best Specialist Feature Found:\n",
      "   Feature #11722\n",
      "   Category: Social/Emoji\n",
      "   Max activation: 19.02\n",
      "   Strong activations (>5.0) in Social/Emoji: 10/10\n",
      "   Strong activations (>5.0) outside Social/Emoji: 2/60\n",
      "   Specialist score: 8\n",
      "\n",
      "ğŸ”— Explore on Neuronpedia:\n",
      "   https://neuronpedia.org/gpt2-small/6-res-jb/11722\n",
      "\n",
      "ğŸ“Š Summary: Found 7/7 categories with true specialist features\n",
      "\n",
      "ğŸ’­ Interpretation:\n",
      "   - STRONGEST feature: Highest peak activation (may still be general)\n",
      "   - FREQUENT feature: Activates consistently across many texts\n",
      "   - SELECTIVE feature: High activation but only on specific text types\n",
      "   - CATEGORY ANALYSIS: Searches for specialists in each category\n",
      "   - Specialist score = (strong in category) - (strong outside category)\n",
      "   - Positive scores indicate specialization; negative scores indicate generalization\n",
      "\n",
      "   âœ“ Found 7 specialist feature(s) - true category-specific patterns detected!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 7: Discover and Analyze Most Interesting Features\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nğŸ”¬ Finding and Analyzing Most Interesting Features\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# APPROACH 1: Feature with highest max activation (strongest)\n",
    "max_activations = diverse_features.max(dim=0)\n",
    "strongest_feature_idx = max_activations.values.argmax().item()\n",
    "strongest_feature_val = max_activations.values.max().item()\n",
    "\n",
    "print(f\"\\n1ï¸âƒ£  STRONGEST Feature (highest activation value):\")\n",
    "print(f\"   Feature #{strongest_feature_idx}\")\n",
    "print(f\"   Max activation: {strongest_feature_val:.2f}\")\n",
    "\n",
    "# APPROACH 2: Most frequently active feature\n",
    "feature_frequency = (diverse_features > 0).sum(dim=0)\n",
    "most_frequent_feature_idx = feature_frequency.argmax().item()\n",
    "most_frequent_count = feature_frequency.max().item()\n",
    "\n",
    "print(f\"\\n2ï¸âƒ£  MOST FREQUENT Feature (active in most texts):\")\n",
    "print(f\"   Feature #{most_frequent_feature_idx}\")\n",
    "print(f\"   Active in: {most_frequent_count}/{len(diverse_texts)} texts\")\n",
    "\n",
    "# APPROACH 3: Most selective feature (high activation but rare)\n",
    "activation_threshold = 5.0  # Minimum to be considered a \"strong\" activation\n",
    "strong_activations = (diverse_features > activation_threshold)  # Boolean mask\n",
    "strong_activation_counts = strong_activations.sum(dim=0)  # Count per feature\n",
    "\n",
    "# Find features that activate strongly on at least 1 text\n",
    "has_strong_activation = strong_activation_counts > 0\n",
    "\n",
    "# Among those, find the one active in the FEWEST texts\n",
    "# Set infinite count for features with no strong activations\n",
    "selectivity_counts = strong_activation_counts.clone().float()\n",
    "selectivity_counts[~has_strong_activation] = float('inf')\n",
    "\n",
    "# Get the feature with minimum count (most selective)\n",
    "most_selective_idx = selectivity_counts.argmin().item()\n",
    "selective_max_val = diverse_features[:, most_selective_idx].max().item()\n",
    "selective_count = strong_activation_counts[most_selective_idx].item()\n",
    "total_active_count = (diverse_features[:, most_selective_idx] > 0).sum().item()\n",
    "\n",
    "print(f\"\\n3ï¸âƒ£  MOST SELECTIVE Feature (specialist - high activation, few texts):\")\n",
    "print(f\"   Feature #{most_selective_idx}\")\n",
    "print(f\"   Max activation: {selective_max_val:.2f}\")\n",
    "print(f\"   Strong activations (>{activation_threshold}) in: {selective_count}/{len(diverse_texts)} texts ({100*selective_count/len(diverse_texts):.1f}%)\")\n",
    "print(f\"   Any activation (>0) in: {total_active_count}/{len(diverse_texts)} texts\")\n",
    "\n",
    "# Show which features we'll analyze in detail\n",
    "features_to_analyze = []\n",
    "labels = []\n",
    "\n",
    "if strongest_feature_idx not in features_to_analyze:\n",
    "    features_to_analyze.append(strongest_feature_idx)\n",
    "    labels.append(\"STRONGEST\")\n",
    "    \n",
    "if most_frequent_feature_idx not in features_to_analyze:\n",
    "    features_to_analyze.append(most_frequent_feature_idx)\n",
    "    labels.append(\"MOST FREQUENT\")\n",
    "    \n",
    "if most_selective_idx not in features_to_analyze:\n",
    "    features_to_analyze.append(most_selective_idx)\n",
    "    labels.append(\"MOST SELECTIVE\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Analyzing {len(features_to_analyze)} distinct features in detail:\")\n",
    "for feat_idx, label in zip(features_to_analyze, labels):\n",
    "    print(f\"   â†’ Feature #{feat_idx} ({label})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Analyze each feature in detail\n",
    "for i, feature_idx in enumerate(features_to_analyze):\n",
    "    label = labels[i]\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Feature #{feature_idx} ({label})\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Get activations for this feature\n",
    "    feature_acts = diverse_features[:, feature_idx]\n",
    "    \n",
    "    # Create dataframe sorted by activation\n",
    "    df = pd.DataFrame({\n",
    "        'Text': diverse_texts,\n",
    "        'Activation': feature_acts.numpy(),\n",
    "        'Active': ['âœ“' if act > 0 else 'âœ—' for act in feature_acts]\n",
    "    })\n",
    "    df = df.sort_values('Activation', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 activating texts:\")\n",
    "    display(df.head(10))\n",
    "    \n",
    "    # Statistics\n",
    "    active_count = (feature_acts > 0).sum().item()\n",
    "    mean_act = feature_acts[feature_acts > 0].mean().item() if active_count > 0 else 0\n",
    "    max_act = feature_acts.max().item()\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ Statistics:\")\n",
    "    print(f\"   Active in: {active_count}/{len(diverse_texts)} texts ({100*active_count/len(diverse_texts):.1f}%)\")\n",
    "    print(f\"   Mean activation (when active): {mean_act:.3f}\")\n",
    "    print(f\"   Max activation: {max_act:.3f}\")\n",
    "    \n",
    "    print(f\"\\nğŸ”— Explore on Neuronpedia:\")\n",
    "    print(f\"   https://neuronpedia.org/gpt2-small/6-res-jb/{feature_idx}\")\n",
    "    \n",
    "    if i < len(features_to_analyze) - 1:\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# APPROACH 4: Category-Specific Feature Analysis (by MAX activation)\n",
    "# Look for features that fire VERY STRONGLY on at least one example in each category\n",
    "\n",
    "categories = {\n",
    "    'Python': list(range(0, 10)),\n",
    "    'URLs/Web': list(range(10, 20)),\n",
    "    'Math': list(range(20, 30)),\n",
    "    'Non-English': list(range(30, 40)),\n",
    "    'Social/Emoji': list(range(40, 50)),\n",
    "    'Formal': list(range(50, 60)),\n",
    "    'Conversational': list(range(60, 70))\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\n4ï¸âƒ£  CATEGORY-SPECIFIC FEATURE ANALYSIS (by MAX activation)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"For each category, finding features with highest activation\")\n",
    "print(\"and checking if they specialize in that category (activate strongly\")\n",
    "print(\"inside the category but rarely outside it):\\n\")\n",
    "\n",
    "best_specialist = None\n",
    "best_specialist_score = -1\n",
    "category_specialists = {}  # Store best specialist for each category\n",
    "category_has_specialist = {}  # Track if category has a true specialist\n",
    "\n",
    "for cat_name, indices in categories.items():\n",
    "    print(f\"\\n{cat_name}:\")\n",
    "    cat_features = diverse_features[indices, :]\n",
    "    \n",
    "    # Find features with highest MAX activation in this category\n",
    "    cat_max = cat_features.max(dim=0)\n",
    "    top_features = cat_max.values.topk(5)\n",
    "    \n",
    "    cat_best_specialist = None\n",
    "    cat_best_score = -1\n",
    "    \n",
    "    for rank, (max_val, feat_idx) in enumerate(zip(top_features.values, top_features.indices), 1):\n",
    "        feat_idx_item = feat_idx.item()\n",
    "        \n",
    "        # Check how often it activates STRONGLY (>5.0) outside this category\n",
    "        other_indices = [i for i in range(len(diverse_texts)) if i not in indices]\n",
    "        strong_outside = (diverse_features[other_indices, feat_idx_item] > 5.0).sum().item()\n",
    "        strong_inside = (diverse_features[indices, feat_idx_item] > 5.0).sum().item()\n",
    "        \n",
    "        # Calculate specialist score: strong activations inside - strong activations outside\n",
    "        specialist_score = strong_inside - strong_outside\n",
    "        \n",
    "        # Track the best specialist for this category\n",
    "        if specialist_score > cat_best_score:\n",
    "            cat_best_score = specialist_score\n",
    "            cat_best_specialist = feat_idx_item\n",
    "        \n",
    "        # Track the best specialist overall\n",
    "        if specialist_score > best_specialist_score:\n",
    "            best_specialist_score = specialist_score\n",
    "            best_specialist = {\n",
    "                'idx': feat_idx_item,\n",
    "                'category': cat_name,\n",
    "                'max_val': max_val.item(),\n",
    "                'strong_inside': strong_inside,\n",
    "                'strong_outside': strong_outside,\n",
    "                'score': specialist_score\n",
    "            }\n",
    "        \n",
    "        print(f\"  {rank}. Feature #{feat_idx_item}: max={max_val:.2f}, strong(>5.0) inside={strong_inside}/10, outside={strong_outside}/60\")\n",
    "    \n",
    "    # Store the top feature for this category and whether it's a true specialist\n",
    "    if cat_best_specialist is not None:\n",
    "        category_specialists[cat_name] = cat_best_specialist\n",
    "        category_has_specialist[cat_name] = (cat_best_score > 0)\n",
    "        \n",
    "        if cat_best_score > 0:\n",
    "            print(f\"  âœ“ True specialist found: Feature #{cat_best_specialist} (score: {cat_best_score})\")\n",
    "        else:\n",
    "            print(f\"  âœ— No specialist (top feature activates more outside than inside)\")\n",
    "        \n",
    "        print(f\"  ğŸ”— Top feature for {cat_name}: https://neuronpedia.org/gpt2-small/6-res-jb/{cat_best_specialist}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Summary of findings\n",
    "num_specialists = sum(category_has_specialist.values())\n",
    "\n",
    "if best_specialist and best_specialist['score'] > 0:\n",
    "    print(f\"\\nğŸ¯ Best Specialist Feature Found:\")\n",
    "    print(f\"   Feature #{best_specialist['idx']}\")\n",
    "    print(f\"   Category: {best_specialist['category']}\")\n",
    "    print(f\"   Max activation: {best_specialist['max_val']:.2f}\")\n",
    "    print(f\"   Strong activations (>5.0) in {best_specialist['category']}: {best_specialist['strong_inside']}/10\")\n",
    "    print(f\"   Strong activations (>5.0) outside {best_specialist['category']}: {best_specialist['strong_outside']}/60\")\n",
    "    print(f\"   Specialist score: {best_specialist['score']}\")\n",
    "    print(f\"\\nğŸ”— Explore on Neuronpedia:\")\n",
    "    print(f\"   https://neuronpedia.org/gpt2-small/6-res-jb/{best_specialist['idx']}\")\n",
    "    print(f\"\\nğŸ“Š Summary: Found {num_specialists}/{len(categories)} categories with true specialist features\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  No True Specialist Features Found\")\n",
    "    print(f\"   All features that activate strongly in one category also activate\")\n",
    "    print(f\"   strongly in other categories (negative or zero specialist scores).\")\n",
    "    print(f\"\\nğŸ“Š Summary: 0/{len(categories)} categories have specialist features\")\n",
    "\n",
    "print(\"\\nğŸ’­ Interpretation:\")\n",
    "print(\"   - STRONGEST feature: Highest peak activation (may still be general)\")\n",
    "print(\"   - FREQUENT feature: Activates consistently across many texts\")\n",
    "print(\"   - SELECTIVE feature: High activation but only on specific text types\")\n",
    "print(\"   - CATEGORY ANALYSIS: Searches for specialists in each category\")\n",
    "print(\"   - Specialist score = (strong in category) - (strong outside category)\")\n",
    "print(\"   - Positive scores indicate specialization; negative scores indicate generalization\")\n",
    "\n",
    "if num_specialists == 0:\n",
    "    print(\"\\n   âš ï¸  Key Finding: This SAE did not learn category-specific specialists\")\n",
    "    print(\"   This demonstrates a core challenge in mechanistic interpretability:\")\n",
    "    print(\"   SAEs don't always decompose into clean, monosemantic features for every domain.\")\n",
    "else:\n",
    "    print(f\"\\n   âœ“ Found {num_specialists} specialist feature(s) - true category-specific patterns detected!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f0fa43-a3d9-40ec-b8f4-d6ce59ad91d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 8: Feature Activation Heatmap\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nğŸ“ˆ Creating Feature Activation Heatmap\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Select top 10 features for visualization\n",
    "top_features_to_viz = frequent_features[:10]\n",
    "\n",
    "# Create activation matrix\n",
    "activation_matrix = diverse_features[:, top_features_to_viz].numpy()\n",
    "\n",
    "# Create heatmap\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=activation_matrix,\n",
    "    x=[f\"F{f}\" for f in top_features_to_viz],\n",
    "    y=[text[:30] + \"...\" if len(text) > 30 else text for text in diverse_texts],\n",
    "    colorscale='YlOrRd',\n",
    "    hoverongaps=False,\n",
    "    hovertemplate='Text: %{y}<br>Feature: %{x}<br>Activation: %{z:.2f}<extra></extra>'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Feature Activation Heatmap: Top 10 Features Across Diverse Texts\",\n",
    "    xaxis_title=\"Feature ID\",\n",
    "    yaxis_title=\"Text Sample\",\n",
    "    height=800,\n",
    "    width=900\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ Darker colors = stronger activation\")\n",
    "print(\"   Look for patterns: which text types activate which features?\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nğŸ“Š Creating Feature Activation Heatmap For Top Features Only\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Select features to visualize (mix of different types)\n",
    "features_to_viz = []\n",
    "feature_labels = []\n",
    "\n",
    "# Add the interesting features we found\n",
    "if strongest_feature_idx not in features_to_viz:\n",
    "    features_to_viz.append(strongest_feature_idx)\n",
    "    feature_labels.append(f\"#{strongest_feature_idx} (Strongest)\")\n",
    "\n",
    "if most_frequent_feature_idx not in features_to_viz:\n",
    "    features_to_viz.append(most_frequent_feature_idx)\n",
    "    feature_labels.append(f\"#{most_frequent_feature_idx} (Frequent)\")\n",
    "\n",
    "if most_selective_idx not in features_to_viz:\n",
    "    features_to_viz.append(most_selective_idx)\n",
    "    feature_labels.append(f\"#{most_selective_idx} (Selective)\")\n",
    "\n",
    "# Add the best specialist from each category (if they're different)\n",
    "for cat_name, feat_idx in category_specialists.items():\n",
    "    if feat_idx not in features_to_viz:\n",
    "        features_to_viz.append(feat_idx)\n",
    "        feature_labels.append(f\"#{feat_idx} ({cat_name})\")\n",
    "\n",
    "print(f\"\\nVisualizing {len(features_to_viz)} features across {len(diverse_texts)} texts\")\n",
    "print(f\"Features: {features_to_viz}\")\n",
    "\n",
    "# Create activation matrix for these features\n",
    "activation_matrix = []\n",
    "for feat_idx in features_to_viz:\n",
    "    activations = diverse_features[:, feat_idx].numpy()\n",
    "    activation_matrix.append(activations)\n",
    "\n",
    "activation_matrix = np.array(activation_matrix)\n",
    "\n",
    "# Create text labels with category prefixes\n",
    "text_labels = []\n",
    "categories_ordered = {\n",
    "    'Python': list(range(0, 10)),\n",
    "    'URLs/Web': list(range(10, 20)),\n",
    "    'Math': list(range(20, 30)),\n",
    "    'Non-English': list(range(30, 40)),\n",
    "    'Social/Emoji': list(range(40, 50)),\n",
    "    'Formal': list(range(50, 60)),\n",
    "    'Conversational': list(range(60, 70))\n",
    "}\n",
    "\n",
    "for i, text in enumerate(diverse_texts):\n",
    "    # Find which category this text belongs to\n",
    "    cat_label = \"\"\n",
    "    for cat_name, indices in categories_ordered.items():\n",
    "        if i in indices:\n",
    "            cat_label = cat_name[:4]  # First 4 letters\n",
    "            break\n",
    "    \n",
    "    # Truncate text for label\n",
    "    text_short = text[:40] + \"...\" if len(text) > 40 else text\n",
    "    text_labels.append(f\"[{cat_label}] {text_short}\")\n",
    "\n",
    "# Create heatmap\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=activation_matrix,\n",
    "    x=text_labels,\n",
    "    y=feature_labels,\n",
    "    colorscale='Viridis',\n",
    "    hoverongaps=False,\n",
    "    hovertemplate='Feature: %{y}<br>Text: %{x}<br>Activation: %{z:.2f}<extra></extra>'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Feature Activation Patterns Across Text Categories - Stongest, Most Active, and Most Selective Features Only',\n",
    "    xaxis_title='Texts (grouped by category)',\n",
    "    yaxis_title='Features',\n",
    "    height=max(400, len(features_to_viz) * 60),\n",
    "    width=1400,\n",
    "    xaxis={'tickangle': -45, 'tickfont': {'size': 8}},\n",
    "    yaxis={'tickfont': {'size': 10}}\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ Interpretation:\")\n",
    "print(\"   - Darker colors = higher activation\")\n",
    "print(\"   - Look for vertical bands (features that activate on specific categories)\")\n",
    "print(\"   - Look for horizontal bands (texts that activate many features)\")\n",
    "print(\"   - Category prefixes: Pyth=Python, URLs=URLs/Web, Math=Math, Non-=Non-English,\")\n",
    "print(\"                        Soci=Social/Emoji, Form=Formal, Conv=Conversational\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1c69c8-4e52-4b21-82f5-589f26e46357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 9: Build Interactive Feature Explorer Function\n",
    "# ============================================================================\n",
    "\n",
    "def explore_feature(feature_idx, texts_to_test=None):\n",
    "    \"\"\"\n",
    "    Explore a specific feature by testing it on custom texts\n",
    "    \n",
    "    Args:\n",
    "        feature_idx: Feature number to explore\n",
    "        texts_to_test: List of texts to test (optional, uses diverse_texts if None)\n",
    "    \"\"\"\n",
    "    if texts_to_test is None:\n",
    "        texts_to_test = diverse_texts\n",
    "    \n",
    "    print(f\"\\nğŸ” Exploring Feature #{feature_idx}\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"ğŸ”— Neuronpedia: https://neuronpedia.org/gpt2-small/6-res-jb/{feature_idx}\")\n",
    "    print()\n",
    "    \n",
    "    results = []\n",
    "    for text in texts_to_test:\n",
    "        # Get activation\n",
    "        logits, cache = model.run_with_cache(text)\n",
    "        acts = cache[hook_name][0, -1, :]\n",
    "        \n",
    "        # Get features\n",
    "        with torch.no_grad():\n",
    "            features = sae.encode(acts.unsqueeze(0))\n",
    "            activation_val = features[0, feature_idx].item()\n",
    "        \n",
    "        results.append((text, activation_val))\n",
    "    \n",
    "    # Sort by activation\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"Top Activating Texts:\")\n",
    "    for i, (text, val) in enumerate(results[:10], 1):\n",
    "        print(f\"  {i}. [{val:6.2f}] {text}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test it with the STRONGEST feature we discovered in Cell 7\n",
    "print(\"Testing the explorer function with dynamically discovered feature:\")\n",
    "print(f\"Using Feature #{strongest_feature_idx} (strongest from Cell 7)\")\n",
    "explore_feature(strongest_feature_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d018fbc-ec40-4309-8491-10bc29e1ef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 10: Feature Co-Activation Analysis\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nğŸ¤ Feature Co-Activation Analysis\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# For each text type category, which features activate together?\n",
    "print(\"\\nAnalyzing which features tend to activate together...\\n\")\n",
    "\n",
    "# Get top 5 features for each text type category\n",
    "categories = {\n",
    "    'Geography': diverse_texts[0:3],\n",
    "    'Code': diverse_texts[3:6],\n",
    "    'Legal': diverse_texts[6:9],\n",
    "    'Casual': diverse_texts[9:12],\n",
    "    'Literature': diverse_texts[12:15],\n",
    "    'News': diverse_texts[15:18],\n",
    "}\n",
    "\n",
    "for category, category_texts in categories.items():\n",
    "    print(f\"\\n{category} Texts:\")\n",
    "    \n",
    "    # Get features for this category\n",
    "    category_feature_counts = torch.zeros(sae.cfg.d_sae)\n",
    "    \n",
    "    for text in category_texts:\n",
    "        idx = diverse_texts.index(text)\n",
    "        active_features = (diverse_features[idx] > 0)\n",
    "        category_feature_counts += active_features.float()\n",
    "    \n",
    "    # Get most common features for this category\n",
    "    top_5 = category_feature_counts.topk(5)\n",
    "    \n",
    "    print(f\"  Most common features:\")\n",
    "    for feat_idx, count in zip(top_5.indices, top_5.values):\n",
    "        print(f\"    Feature #{feat_idx.item()}: active in {int(count.item())}/{len(category_texts)} texts\")\n",
    "        print(f\"      â†’ https://neuronpedia.org/gpt2-small/6-res-jb/{feat_idx.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f277d78-fcb8-4b59-91de-0efba0296a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 11: Find Features for a Specific Concept (with Activation Strength)\n",
    "# ============================================================================\n",
    "\n",
    "def find_features_for_concept(concept_texts, top_k=5, activation_threshold=5.0):\n",
    "    \"\"\"\n",
    "    Find features that consistently activate STRONGLY for a concept\n",
    "    \n",
    "    Args:\n",
    "        concept_texts: List of texts representing a concept\n",
    "        top_k: Number of top features to return\n",
    "        activation_threshold: Minimum activation to be considered \"strong\"\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ¯ Finding features for concept (analyzing {len(concept_texts)} texts)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Accumulate feature activations with strength weighting\n",
    "    accumulated_strong = torch.zeros(sae.cfg.d_sae)  # Count of strong activations\n",
    "    accumulated_strength = torch.zeros(sae.cfg.d_sae)  # Sum of activation strengths\n",
    "    \n",
    "    for text in concept_texts:\n",
    "        print(f\"  Analyzing: '{text[:50]}...'\")\n",
    "        \n",
    "        # Get features\n",
    "        logits, cache = model.run_with_cache(text)\n",
    "        acts = cache[hook_name][0, -1, :]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            features = sae.encode(acts.unsqueeze(0))\n",
    "            \n",
    "            # Count strong activations (above threshold)\n",
    "            accumulated_strong += (features[0] > activation_threshold).float()\n",
    "            \n",
    "            # Also accumulate total strength for mean calculation\n",
    "            accumulated_strength += features[0]\n",
    "    \n",
    "    # Calculate mean activation strength (only for texts where it activated)\n",
    "    mean_strength = torch.zeros_like(accumulated_strength)\n",
    "    active_count = (accumulated_strength > 0).float()\n",
    "    mean_strength[active_count > 0] = accumulated_strength[active_count > 0] / active_count[active_count > 0]\n",
    "    \n",
    "    # Find features with most strong activations\n",
    "    top_features_by_strong = accumulated_strong.topk(top_k)\n",
    "    \n",
    "    # Also find features with highest mean activation strength\n",
    "    top_features_by_mean = mean_strength.topk(top_k)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Top {top_k} features by STRONG activation consistency (>{activation_threshold}):\")\n",
    "    print(\"-\" * 70)\n",
    "    for rank, (count, idx) in enumerate(zip(top_features_by_strong.values, top_features_by_strong.indices), 1):\n",
    "        mean_act = mean_strength[idx].item()\n",
    "        consistency = (count.item() / len(concept_texts)) * 100\n",
    "        print(f\"  {rank}. Feature #{idx.item()}\")\n",
    "        print(f\"     Strong activations: {int(count.item())}/{len(concept_texts)} texts ({consistency:.0f}%)\")\n",
    "        print(f\"     Mean activation (all texts): {mean_act:.2f}\")\n",
    "        print(f\"     â†’ https://neuronpedia.org/gpt2-small/6-res-jb/{idx.item()}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Top {top_k} features by MEAN activation strength:\")\n",
    "    print(\"-\" * 70)\n",
    "    for rank, (mean_val, idx) in enumerate(zip(top_features_by_mean.values, top_features_by_mean.indices), 1):\n",
    "        strong_count = accumulated_strong[idx].item()\n",
    "        consistency = (strong_count / len(concept_texts)) * 100\n",
    "        print(f\"  {rank}. Feature #{idx.item()}\")\n",
    "        print(f\"     Mean activation: {mean_val:.2f}\")\n",
    "        print(f\"     Strong activations (>{activation_threshold}): {int(strong_count)}/{len(concept_texts)} ({consistency:.0f}%)\")\n",
    "        print(f\"     â†’ https://neuronpedia.org/gpt2-small/6-res-jb/{idx.item()}\")\n",
    "    \n",
    "    print(\"\\nğŸ’¡ Interpretation:\")\n",
    "    print(f\"   - 'Strong activation consistency': Features that activate >{activation_threshold} most frequently\")\n",
    "    print(f\"   - 'Mean activation strength': Features with highest average activation\")\n",
    "    print(f\"   - True specialists should score high on BOTH metrics\")\n",
    "    \n",
    "    return top_features_by_strong.indices.tolist()\n",
    "\n",
    "# Test: Find features for \"code/programming\"\n",
    "code_texts = [\n",
    "    \"def function(x): return x + 1\",\n",
    "    \"import numpy as np\",\n",
    "    \"for i in range(10): print(i)\",\n",
    "    \"class MyClass: pass\",\n",
    "    \"if __name__ == '__main__':\",\n",
    "]\n",
    "\n",
    "print(\"\\nExample: Finding features for 'Code/Programming' concept\")\n",
    "code_features = find_features_for_concept(code_texts, top_k=5, activation_threshold=5.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becfac7d-a3b7-456c-bf32-e957be850008",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
